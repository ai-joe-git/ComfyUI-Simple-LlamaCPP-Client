# ComfyUI-Simple-LlamaCPP-Client

A lightweight custom node for **ComfyUI** that connects directly to a local **llama.cpp OpenAI-compatible server**.

It supports:

- ðŸ’¬ Chat completions (`/v1/chat/completions`)
- ðŸ–¼ Vision models (image + prompt input)
- âš¡ Streaming mode (SSE token accumulation)
- ðŸ§  Clean Answer + Thinking separation
- ðŸ“¦ Optional JSON-only output mode
- ðŸ”‘ Optional API key authentication
- ðŸŽ› Minimal design (server-side parameters stay server-side)

---

## âœ¨ Features

âœ… Works with any OpenAI-compatible llama.cpp server  
âœ… Supports **system prompt + user prompt**  
âœ… Optional **image input** for multimodal models  
âœ… Outputs: Answer, Thinking, JSON, Raw, Model Used  
âœ… Auto-detects model from `/v1/models`  
âœ… Clean dropdown UI (no ugly free-text params)

---

## ðŸ“¦ Installation

```bash
cd ComfyUI/custom_nodes
git clone https://github.com/ai-joe-git/ComfyUI-Simple-LlamaCPP-Client.git
```

Restart ComfyUI.

---

## ðŸš€ Server Example

```bat
llama-server.exe ^
  -m Ministral-3-8B-Instruct.gguf ^
  --host 127.0.0.1 ^
  --port 8082 ^
  --mmproj mmproj.gguf ^
  -c 8192
```

---

## ðŸ§© Node Inputs

| Input | Description |
|------|------------|
| `server_url` | llama.cpp server URL (default: `http://127.0.0.1:8082`) |
| `prompt` | User message text |
| `system_prompt` | Optional system instruction |
| `image` | Optional IMAGE input (vision models) |
| `api_key` | Optional Bearer token |

### Model Selection

| Input | Description |
|------|------------|
| `model_mode` | Dropdown: `auto` / `custom` |
| `model_override` | Only used if `model_mode = custom` |

### Stop Control

| Input | Description |
|------|------------|
| `stop_mode` | Dropdown: `none`, `preset:common_eot`, `preset:triple_hash`, `custom` |
| `stop_custom` | Used only if stop_mode = custom |

### Text Cleanup

| Input | Description |
|------|------------|
| `text_postprocess` | Dropdown: `fix_mojibake`, `none`, `ascii_quotes`, `fix_mojibake+ascii_quotes` |

(Default fixes `HereÃ¢Â€Â™s` â†’ `Hereâ€™s`)

---

## ðŸ“¤ Node Outputs

| Output | Description |
|-------|------------|
| `answer` | Final cleaned answer |
| `thinking` | Reasoning if provided |
| `json` | Parsed JSON output (if enabled) |
| `raw` | Full raw server response |
| `model_used` | Model name used |

---

## ðŸ“œ License

MIT License.
