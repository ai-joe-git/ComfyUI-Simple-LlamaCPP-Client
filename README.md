# ComfyUI-Simple-LlamaCPP-Client
A lightweight custom node for **ComfyUI** that connects to a local **llama.cpp OpenAI-compatible server** with support for chat, vision, streaming, JSON mode, and clean answer/thinking separation.
